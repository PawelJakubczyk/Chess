{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9nVM6KnM2r0sIEM2t0YiK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PawelJakubczyk/Chess/blob/master/pyspark_crud_read.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Liblary"
      ],
      "metadata": {
        "id": "IjO7po58ZMLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op_JYUA9Yukr",
        "outputId": "38f60ef2-3d16-479c-d9ca-13db27149805"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Using cached pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=9ddb01b53e1538ef5eee46b23f41c856402182b8bc6bf345a0c2a12374b1dc04\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creat Pyspark DF"
      ],
      "metadata": {
        "id": "z2dUaWy1kzHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "qR-V3bvhZRJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZmHIGJsJYBCy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Spark Session"
      ],
      "metadata": {
        "id": "RPigNF2tZVk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "YRckIPtCYxvm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Schema"
      ],
      "metadata": {
        "id": "3ydjdUXQZfcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_schema = StructType(\n",
        "    [\n",
        "        StructField(\"ID\", StringType()),\n",
        "        StructField(\"Subsector\", StringType()),\n",
        "        StructField(\"Category\", StringType()),\n",
        "        StructField(\"Brand\", StringType()),\n",
        "        StructField(\"Material_ID\", IntegerType()),\n",
        "        StructField(\"Description\", StringType()),\n",
        "        StructField(\"Plant_Code\", StringType()),\n",
        "        StructField(\"Plant_Name\", StringType()),\n",
        "        StructField(\"Validity_Date_From\", DateType()),\n",
        "        StructField(\"Validity_Date_To\", DateType()),\n",
        "        StructField(\"Modification_Date\", DateType()),\n",
        "        StructField(\"Status\", StringType()),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1YSnZHxtZga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Empty DF"
      ],
      "metadata": {
        "id": "fR2euXSTyap7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "df_empty = spark.createDataFrame(emptyRDD, custom_schema)\n",
        "\n",
        "df_empty = spark.createDataFrame([], custom_schema)"
      ],
      "metadata": {
        "id": "9tkyUMYUyaZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DF from variables"
      ],
      "metadata": {
        "id": "d28LZDdOy2pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# list  of college data with two lists\n",
        "data = [[\"node.js\", \"dbms\", \"integration\"],\n",
        "        [\"jsp\", \"SQL\", \"trigonometry\"],\n",
        "        [\"php\", \"oracle\", \"statistics\"],\n",
        "        [\".net\", \"db2\", \"Machine Learning\"]]\n",
        "\n",
        "# giving column names of dataframe\n",
        "columns = [\"Web Technologies\", \"Data bases\", \"Maths\"]\n",
        "\n",
        "# creating a dataframe\n",
        "dataframe = spark.createDataFrame(data, columns, schema=custom_schema)"
      ],
      "metadata": {
        "id": "bEkYjFVjy-Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Craete DF from file"
      ],
      "metadata": {
        "id": "FaxAUlIFZcrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### xlsx"
      ],
      "metadata": {
        "id": "jUOT7uovf-FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xlsx_file_path = \"path/to/your/xlsx/file.xlsx\"\n",
        "sheet_name = \"name_of_your_excel_sheet\"\n",
        "\n",
        "df_xlsx_pd = pd.read_excel(xlsx_file_path, sheet_name=sheet_name, inferSchema=True, schema=custom_schema)\n",
        "df_xlsx = spark.createDataFrame(df_xlsx_pd)\n",
        "\n",
        "df_xlsx = (spark.read.format(\"com.crealytics.spark.excel\")\n",
        "    .option(\"useHeader\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"dataAddress\", f\"'{sheet_name}'!\")\n",
        "    .schema(custom_schema)\n",
        "    .load(xlsx_file_path))"
      ],
      "metadata": {
        "id": "PAYZmF9LY31h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### csv"
      ],
      "metadata": {
        "id": "_a-dj38hf2iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"path/to/your/csv/file.csv\"\n",
        "df_csv = spark.read.csv(csv_file_path, schema=custom_schema, header=True, inferSchema=True)\n",
        "\n",
        "df_csv = (spark.read.format(\"csv\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .schema(custom_schema)\n",
        "    .load(csv_file_path))"
      ],
      "metadata": {
        "id": "5lqVnve8f4qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### json"
      ],
      "metadata": {
        "id": "3WQN-ENPhZdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"path/to/your/json/file.json\"\n",
        "\n",
        "df_json = spark.read.json(json_file_path, schema=custom_schema, header=True, inferSchema=True)\n",
        "\n",
        "df_json = (spark.read.format(\"json\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .schema(custom_schema)\n",
        "    .load(json_file_path))"
      ],
      "metadata": {
        "id": "uS69Pn5khY77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### parquet"
      ],
      "metadata": {
        "id": "BA2d6Jcei7zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_file_path = \"path/to/your/json/file.json\"\n",
        "\n",
        "df_hist_del = spark.read.parquet(parquet_file_path)\n",
        "df_read_history = spark.read.format(\"parquet\").load(parquet_file_path)"
      ],
      "metadata": {
        "id": "M6vg0OeDi-yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### delta"
      ],
      "metadata": {
        "id": "pVJ7-9gOmIaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_files_path = \"path/to/your/delta/folder\"\n",
        "\n",
        "df_delta = spark.read.format(\"delta\").load(delta_files_path)\n",
        "df_delta = spark.read.delta.load(delta_files_path)"
      ],
      "metadata": {
        "id": "YT6WwbUHmKGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### all format"
      ],
      "metadata": {
        "id": "KZ8RKtxAnSxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"path/to/your/file_or_folder\"\n",
        "\n",
        "def create_df(df_format:str, path:str, sheet_name:str = \"\", custom_schema = \"\"):\n",
        "    formats_list = [\"csv\", \"xlsx\", \"json\", \"parquet\", \"delta\"]\n",
        "    df_options = \"\"\n",
        "\n",
        "    if df_format not in formats_list:\n",
        "        raise ValueError(f\"the function only supports selected formats: {formats_list}\")\n",
        "\n",
        "    if df_format not in [\"parquet\", \"delta\"]:\n",
        "        if not custom_schema:\n",
        "            raise ValueError(\"the selected df_format does not store a schema, you must provide the schema argument\")\n",
        "        df_options += '.option(\"useHeader\", \"true\").option(\"inferSchema\", \"true\").schema(custom_schema)'\n",
        "\n",
        "        if df_format == \"xlsx\":\n",
        "            df_format = \"com.crealytics.spark.excel\"\n",
        "            if sheet_name == '':\n",
        "                raise ValueError(\"for the xlsx df_format, the sheets variable is required\")\n",
        "            ptions += f\"\"\".option(\"dataAddress\", f\"'{sheet_name}'!\")\"\"\"\n",
        "\n",
        "    df = eval(f'spark.read.format({df_format}).{df_options}.load({path})')\n",
        "    # df = spark.read.format(df_format).options(eval(df_options)).load(path)\n",
        "    return df"
      ],
      "metadata": {
        "id": "d92QhruMnZh9"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}